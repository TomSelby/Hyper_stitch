{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e87cd56-f3ec-40ea-9177-0ed3f145280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import hyperspy.api as hs\n",
    "import kornia.feature as KF\n",
    "import kornia as K\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from kornia_moons.viz import *\n",
    "from kornia_moons.feature import *\n",
    "%matplotlib qt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c31cffb-2294-4b48-9dfc-6634bf82dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname1 = \"kn_church-2.jpg\"\n",
    "fname2 = \"kn_church-8.jpg\"\n",
    "\n",
    "img0 = K.io.load_image(fname1, K.io.ImageLoadType.RGB32)[None]\n",
    "img1 = K.io.load_image(fname2, K.io.ImageLoadType.RGB32)[None]\n",
    "img0 = K.geometry.resize(img0, (600, 375), antialias=True)\n",
    "img1 = K.geometry.resize(img1, (600, 375), antialias=True)\n",
    "\n",
    "input_dict = {\n",
    "    \"image0\": K.color.rgb_to_grayscale(img0),  # LofTR works on grayscale images only\n",
    "    \"image1\": K.color.rgb_to_grayscale(img1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be8e569-63fb-4628-985d-06437ec252f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_torch_tensor(npy):\n",
    "    npy = npy - np.amin(npy)\n",
    "    npy = 255*(npy/np.amax(npy))\n",
    "    tensor = np.zeros((1,1,np.shape(npy)[0],np.shape(npy)[1]))\n",
    "    tensor[0,0] = npy\n",
    "    tensor = torch.FloatTensor(tensor)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49f4239d-9433-4c3f-ad60-e2905854d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img0 = numpy_to_torch_tensor(np.load(r\"C:\\Users\\tas72\\Documents\\PhD\\dg606\\20230624\\Stitching_and_corr\\test\\20230624_100334_vbf.npy\"))\n",
    "img1 = numpy_to_torch_tensor(np.load(r\"C:\\Users\\tas72\\Documents\\PhD\\dg606\\20230624\\Stitching_and_corr\\test\\20230624_100824_vbf.npy\"))\n",
    "input_dict = {\"image0\":img0,\n",
    "             \"image1\":img1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8c3bd47-f516-45c1-873c-4972bbdd9ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a738bd7a10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.imshow(K.tensor_to_image(img1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7091a635-b96f-42ba-bf03-8ab7b174df6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = KF.LoFTR(pretrained=\"indoor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26765585-63f8-4d32-bd5f-3fbd247d4c58",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m----> 2\u001b[0m     correspondences \u001b[38;5;241m=\u001b[39m \u001b[43mmatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\20230721\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\.conda\\envs\\20230721\\Lib\\site-packages\\kornia\\feature\\hardnet.py:88\u001b[0m, in \u001b[0;36mHardNet.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m---> 88\u001b[0m     \u001b[43mKORNIA_CHECK_SHAPE\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     x_norm: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_input(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m     90\u001b[0m     x_features: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures(x_norm)\n",
      "File \u001b[1;32m~\\.conda\\envs\\20230721\\Lib\\site-packages\\kornia\\core\\check.py:57\u001b[0m, in \u001b[0;36mKORNIA_CHECK_SHAPE\u001b[1;34m(x, shape)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     shape_to_check \u001b[38;5;241m=\u001b[39m shape\n\u001b[1;32m---> 57\u001b[0m     x_shape_to_check \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x_shape_to_check) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(shape_to_check):\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m shape must be [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    correspondences = matcher(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "282771fe-8c97-4467-8fe4-1c6f844d182f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keypoints0\n",
      "keypoints1\n",
      "confidence\n",
      "batch_indexes\n"
     ]
    }
   ],
   "source": [
    "for k, v in correspondences.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dbfadab-5c8f-4937-b4bc-107f65c79005",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkpts0 = correspondences[\"keypoints0\"].cpu().numpy()\n",
    "mkpts1 = correspondences[\"keypoints1\"].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da7b2543-2a11-4c2e-93c3-c1b382927672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 2), dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mkpts0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "53365358-d0e0-481b-827b-1460e2652eb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\calib3d\\src\\usac\\ransac_solvers.cpp:1046: error: (-2:Unspecified error) in function 'auto __cdecl cv::usac::mergePoints::<lambda_1ca39094813444db980908f76b22a0b5>::operator ()(class cv::Mat &,int) const'\n> Invalid dimension of point (expected: 'points.cols >= pt_dim'), where\n>     'points.cols' is 0\n> must be greater than or equal to\n>     'pt_dim' is 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Fm, inliers \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindFundamentalMat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmkpts0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmkpts1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUSAC_MAGSAC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.999\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m inliers \u001b[38;5;241m=\u001b[39m inliers \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\calib3d\\src\\usac\\ransac_solvers.cpp:1046: error: (-2:Unspecified error) in function 'auto __cdecl cv::usac::mergePoints::<lambda_1ca39094813444db980908f76b22a0b5>::operator ()(class cv::Mat &,int) const'\n> Invalid dimension of point (expected: 'points.cols >= pt_dim'), where\n>     'points.cols' is 0\n> must be greater than or equal to\n>     'pt_dim' is 2\n"
     ]
    }
   ],
   "source": [
    "Fm, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.5, 0.999, 100000)\n",
    "inliers = inliers > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "22c4ff51-c0db-41c8-8a83-a3d77d5bd323",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_LAF_matches(\n",
    "    KF.laf_from_center_scale_ori(\n",
    "        torch.from_numpy(mkpts0).view(1, -1, 2),\n",
    "        torch.ones(mkpts0.shape[0]).view(1, -1, 1, 1),\n",
    "        torch.ones(mkpts0.shape[0]).view(1, -1, 1),\n",
    "    ),\n",
    "    KF.laf_from_center_scale_ori(\n",
    "        torch.from_numpy(mkpts1).view(1, -1, 2),\n",
    "        torch.ones(mkpts1.shape[0]).view(1, -1, 1, 1),\n",
    "        torch.ones(mkpts1.shape[0]).view(1, -1, 1),\n",
    "    ),\n",
    "    torch.arange(mkpts0.shape[0]).view(-1, 1).repeat(1, 2),\n",
    "    K.tensor_to_image(img0),\n",
    "    K.tensor_to_image(img1),\n",
    "    inliers,\n",
    "    draw_dict={\"inlier_color\": (0.2, 1, 0.2), \"tentative_color\": None, \"feature_color\": (0.2, 0.5, 1), \"vertical\": False},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14356e3-bd35-47a2-92f3-d9b77d4df7c0",
   "metadata": {},
   "source": [
    "Using SIFT and concerting it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c78dda0-4aae-4a48-afb7-3c93e29c5432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_kpts(lafs1, lafs2, idxs):\n",
    "    src_pts = KF.get_laf_center(lafs1).view(-1, 2)[idxs[:, 0]].detach().cpu().numpy()\n",
    "    dst_pts = KF.get_laf_center(lafs2).view(-1, 2)[idxs[:, 1]].detach().cpu().numpy()\n",
    "    return src_pts, dst_pts\n",
    "    \n",
    "def sift_matching(fname1, fname2):\n",
    "    img1 = cv2.cvtColor(cv2.imread(fname1), cv2.COLOR_BGR2RGB)\n",
    "    img2 = cv2.cvtColor(cv2.imread(fname2), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # OpenCV SIFT\n",
    "    sift = cv2.SIFT_create(8000)\n",
    "    kps1, descs1 = sift.detectAndCompute(img1, None)\n",
    "    kps2, descs2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "    # Converting to kornia for matching via AdaLAM\n",
    "    lafs1 = laf_from_opencv_SIFT_kpts(kps1)\n",
    "    lafs2 = laf_from_opencv_SIFT_kpts(kps2)\n",
    "    dists, idxs = KF.match_adalam(\n",
    "        torch.from_numpy(descs1), torch.from_numpy(descs2), lafs1, lafs2, hw1=img1.shape[:2], hw2=img2.shape[:2]\n",
    "    )\n",
    "\n",
    "    # Converting back to kornia via to use OpenCV MAGSAC++\n",
    "    tentatives = cv2_matches_from_kornia(dists, idxs)\n",
    "    src_pts = np.float32([kps1[m.queryIdx].pt for m in tentatives]).reshape(-1, 2)\n",
    "    dst_pts = np.float32([kps2[m.trainIdx].pt for m in tentatives]).reshape(-1, 2)\n",
    "\n",
    "    F, inliers_mask = cv2.findFundamentalMat(src_pts, dst_pts, cv2.USAC_MAGSAC, 0.25, 0.999, 100000)\n",
    "\n",
    "    # Drawing matches using kornia_moons\n",
    "    draw_LAF_matches(\n",
    "        lafs1,\n",
    "        lafs2,\n",
    "        idxs,\n",
    "        img1,\n",
    "        img2,\n",
    "        inliers_mask,\n",
    "        draw_dict={\"inlier_color\": (0.2, 1, 0.2), \"tentative_color\": None, \"feature_color\": None, \"vertical\": False},\n",
    "    )\n",
    "    print(f\"{inliers_mask.sum()} inliers found\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7d9148b-35cf-4700-b7fd-38f08b8d7d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 inliers found\n"
     ]
    }
   ],
   "source": [
    "fname1 = \"kn_church-2.jpg\"\n",
    "fname2 = \"kn_church-8.jpg\"\n",
    "sift_matching(fname1, fname2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e8091555-22cb-4220-b3f5-419c5deb5cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_kpts(lafs1, lafs2, idxs):\n",
    "    src_pts = KF.get_laf_center(lafs1).view(-1, 2)[idxs[:, 0]].detach().cpu().numpy()\n",
    "    dst_pts = KF.get_laf_center(lafs2).view(-1, 2)[idxs[:, 1]].detach().cpu().numpy()\n",
    "    return src_pts, dst_pts\n",
    "\n",
    "\n",
    "def sift_korniadesc_matching(fname1, fname2, descriptor):\n",
    "    timg1 = K.io.load_image(fname1, K.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\n",
    "    timg2 = K.io.load_image(fname2, K.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\n",
    "\n",
    "    sift = OpenCVDetectorKornia(cv2.SIFT_create(8000))\n",
    "    local_feature = KF.LocalFeature(sift, KF.LAFDescriptor(descriptor))\n",
    "\n",
    "    lafs1, resps1, descs1 = local_feature(K.color.rgb_to_grayscale(timg1))\n",
    "    lafs2, resps2, descs2 = local_feature(K.color.rgb_to_grayscale(timg2))\n",
    "\n",
    "    dists, idxs = KF.match_adalam(descs1[0], descs2[0], lafs1, lafs2, hw1=timg1.shape[2:], hw2=timg2.shape[2:])\n",
    "\n",
    "    src_pts, dst_pts = get_matching_kpts(lafs1, lafs2, idxs)\n",
    "    F, inliers_mask = cv2.findFundamentalMat(src_pts, dst_pts, cv2.USAC_MAGSAC, 0.25, 0.999, 100000)\n",
    "    draw_LAF_matches(\n",
    "        lafs1,\n",
    "        lafs2,\n",
    "        idxs,\n",
    "        K.tensor_to_image(timg1),\n",
    "        K.tensor_to_image(timg2),\n",
    "        inliers_mask,\n",
    "        draw_dict={\"inlier_color\": (0.2, 1, 0.2), \"tentative_color\": None, \"feature_color\": None, \"vertical\": False},\n",
    "    )\n",
    "    print(f\"{inliers_mask.sum()} inliers found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a6776cd1-c08c-46fe-a493-390e05f9d314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 inliers found\n"
     ]
    }
   ],
   "source": [
    "mkd = KF.MKDDescriptor(32)\n",
    "with torch.inference_mode():\n",
    "    sift_korniadesc_matching(fname1, fname2, mkd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8cddb7a7-31de-451b-b944-9c11963b8d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/vbalnt/tfeat/raw/master/pretrained-models/tfeat-liberty.params\" to C:\\Users\\tas72/.cache\\torch\\hub\\checkpoints\\tfeat-liberty.params\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2.29M/2.29M [00:00<00:00, 85.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 inliers found\n"
     ]
    }
   ],
   "source": [
    "tfeat = KF.TFeat(True)\n",
    "with torch.inference_mode():\n",
    "    sift_korniadesc_matching(fname1, fname2, tfeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd20eea9-b144-4f42-8dc5-1ea3ce5bc9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 inliers found\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "hardnet = KF.HardNet(True).eval()\n",
    "with torch.inference_mode():\n",
    "    sift_korniadesc_matching(fname1, fname2, hardnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8071a424-b01c-407e-ae6d-634070421f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def siftaffnet_korniadesc_matching(fname1, fname2, descriptor):\n",
    "    timg1 = K.io.load_image(fname1, K.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\n",
    "    timg2 = K.io.load_image(fname2, K.io.ImageLoadType.RGB32)[None, ...]  # BxCxHxW\n",
    "\n",
    "    # Magic is here\n",
    "    sift = OpenCVDetectorWithAffNetKornia(cv2.SIFT_create(8000))\n",
    "\n",
    "    local_feature = KF.LocalFeature(sift, KF.LAFDescriptor(descriptor))\n",
    "    with torch.inference_mode():\n",
    "        lafs1, resps1, descs1 = local_feature(K.color.rgb_to_grayscale(timg1))\n",
    "        lafs2, resps2, descs2 = local_feature(K.color.rgb_to_grayscale(timg2))\n",
    "        dists, idxs = KF.match_adalam(descs1[0], descs2[0], lafs1, lafs2, hw1=timg1.shape[2:], hw2=timg2.shape[2:])\n",
    "\n",
    "    src_pts, dst_pts = get_matching_kpts(lafs1, lafs2, idxs)\n",
    "\n",
    "    F, inliers_mask = cv2.findFundamentalMat(src_pts, dst_pts, cv2.USAC_MAGSAC, 0.25, 0.999, 100000)\n",
    "    print(F)\n",
    "    draw_LAF_matches(\n",
    "        lafs1,\n",
    "        lafs2,\n",
    "        idxs,\n",
    "        K.tensor_to_image(timg1),\n",
    "        K.tensor_to_image(timg2),\n",
    "        inliers_mask,\n",
    "        draw_dict={\"inlier_color\": (0.2, 1, 0.2), \"tentative_color\": None, \"feature_color\": None, \"vertical\": False},\n",
    "    )\n",
    "    print(f\"{inliers_mask.sum()} inliers found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06ca4ce2-9e73-455b-9a67-3e44ccdc99a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.50187827e-07 -2.31419734e-06  1.85316246e-03]\n",
      " [-6.50406627e-06 -1.31747622e-06  1.06966896e-02]\n",
      " [ 5.24452351e-03 -4.69812817e-03 -2.41414730e+00]]\n",
      "34 inliers found\n"
     ]
    }
   ],
   "source": [
    "siftaffnet_korniadesc_matching(fname1, fname2, hardnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e95e19b-2eb7-4da6-a1b8-7be23254b221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.85230503e-07 -9.18246490e-07  1.37376044e-03]\n",
      " [-8.98271988e-06 -1.64258660e-06  1.13717952e-02]\n",
      " [ 6.41669411e-03 -4.91509637e-03 -2.54892365e+00]]\n",
      "47 inliers found\n"
     ]
    }
   ],
   "source": [
    "siftaffnet_korniadesc_matching(fname1, fname2, KF.HyNet(True).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a14d94d-42d9-460d-81cf-489230dbc916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.02526735e-06  2.51399907e-07 -1.20717426e-03]\n",
      " [ 1.05014194e-05  1.48716450e-06 -1.14522576e-02]\n",
      " [-6.95772479e-03  5.00638956e-03  2.56243873e+00]]\n",
      "47 inliers found\n"
     ]
    }
   ],
   "source": [
    "siftaffnet_korniadesc_matching(fname1, fname2, KF.SOSNet(True).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eff4e186-429a-42c2-8330-cedc40816969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.55459316e-07  2.04939806e-07 -4.38493907e-04]\n",
      " [ 3.96477866e-06  5.18799795e-07 -4.43864472e-03]\n",
      " [-2.74114707e-03  1.96796122e-03  1.00000000e+00]]\n",
      "37 inliers found\n"
     ]
    }
   ],
   "source": [
    "siftaffnet_korniadesc_matching(fname1, fname2, KF.HardNet8(True).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7574a50d-018f-45ae-b27f-59d0373c9587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://raw.githubusercontent.com/cvlab-epfl/disk/master/depth-save.pth\" to C:\\Users\\tas72/.cache\\torch\\hub\\checkpoints\\depth-save.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4.17M/4.17M [00:00<00:00, 95.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222 tentative matches with DISK AdaLAM\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "fname1 = \"kn_church-2.jpg\"\n",
    "fname2 = \"kn_church-8.jpg\"\n",
    "\n",
    "adalam_config = KF.adalam.get_adalam_default_config()\n",
    "# adalam_config['orientation_difference_threshold'] = None\n",
    "# adalam_config['scale_rate_threshold'] = None\n",
    "adalam_config[\"force_seed_mnn\"] = False\n",
    "adalam_config[\"search_expansion\"] = 16\n",
    "adalam_config[\"ransac_iters\"] = 256\n",
    "\n",
    "\n",
    "img1 = K.io.load_image(fname1, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "img2 = K.io.load_image(fname2, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "\n",
    "num_features = 2048\n",
    "disk = KF.DISK.from_pretrained(\"depth\").to(device)\n",
    "\n",
    "hw1 = torch.tensor(img1.shape[2:], device=device)\n",
    "hw2 = torch.tensor(img2.shape[2:], device=device)\n",
    "\n",
    "match_with_adalam = True\n",
    "\n",
    "with torch.inference_mode():\n",
    "    inp = torch.cat([img1, img2], dim=0)\n",
    "    features1, features2 = disk(inp, num_features, pad_if_not_divisible=True)\n",
    "    kps1, descs1 = features1.keypoints, features1.descriptors\n",
    "    kps2, descs2 = features2.keypoints, features2.descriptors\n",
    "    if match_with_adalam:\n",
    "        lafs1 = KF.laf_from_center_scale_ori(kps1[None], 96 * torch.ones(1, len(kps1), 1, 1, device=device))\n",
    "        lafs2 = KF.laf_from_center_scale_ori(kps2[None], 96 * torch.ones(1, len(kps2), 1, 1, device=device))\n",
    "\n",
    "        dists, idxs = KF.match_adalam(descs1, descs2, lafs1, lafs2, hw1=hw1, hw2=hw2, config=adalam_config)\n",
    "    else:\n",
    "        dists, idxs = KF.match_smnn(descs1, descs2, 0.98)\n",
    "\n",
    "\n",
    "print(f\"{idxs.shape[0]} tentative matches with DISK AdaLAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a13730c5-345e-4d0f-bc14-0fbf3384f1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 inliers with DISK\n"
     ]
    }
   ],
   "source": [
    "def get_matching_keypoints(kp1, kp2, idxs):\n",
    "    mkpts1 = kp1[idxs[:, 0]]\n",
    "    mkpts2 = kp2[idxs[:, 1]]\n",
    "    return mkpts1, mkpts2\n",
    "\n",
    "\n",
    "mkpts1, mkpts2 = get_matching_keypoints(kps1, kps2, idxs)\n",
    "\n",
    "Fm, inliers = cv2.findFundamentalMat(\n",
    "    mkpts1.detach().cpu().numpy(), mkpts2.detach().cpu().numpy(), cv2.USAC_MAGSAC, 1.0, 0.999, 100000\n",
    ")\n",
    "inliers = inliers > 0\n",
    "print(f\"{inliers.sum()} inliers with DISK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c0328a4-b2e3-4097-9ddc-8519c5943cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_LAF_matches(\n",
    "    KF.laf_from_center_scale_ori(kps1[None].cpu()),\n",
    "    KF.laf_from_center_scale_ori(kps2[None].cpu()),\n",
    "    idxs.cpu(),\n",
    "    K.tensor_to_image(img1.cpu()),\n",
    "    K.tensor_to_image(img2.cpu()),\n",
    "    inliers,\n",
    "    draw_dict={\"inlier_color\": (0.2, 1, 0.2), \"tentative_color\": (1, 1, 0.2, 0.3), \"feature_color\": None, \"vertical\": False},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1670b81-552b-4ea7-bc7b-fe1821134b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_matching_keypoints()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
